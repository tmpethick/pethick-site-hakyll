
@article{chengSharpConvergenceRates2019c,
  title = {Sharp {{Convergence Rates}} for {{Langevin Dynamics}} in the {{Nonconvex Setting}}},
  author = {Cheng, Xiang and Chatterji, Niladri S. and {Abbasi-Yadkori}, Yasin and Bartlett, Peter L. and Jordan, Michael I.},
  year = {2019},
  month = feb,
  abstract = {We study the problem of sampling from a distribution where the negative logarithm of the target density is \$L\$-smooth everywhere and \$m\$-strongly convex outside a ball of radius \$R\$, but potentially non-convex inside this ball. We study both overdamped and underdamped Langevin MCMC and prove upper bounds on the time required to obtain a sample from a distribution that is within \$\textbackslash epsilon\$ of the target distribution in \$1\$-Wasserstein distance. For the first-order method (overdamped Langevin MCMC), the time complexity is \$\textbackslash tilde\{\textbackslash mathcal\{O\}\}\textbackslash left(e\^\{cLR\^2\}\textbackslash frac\{d\}\{\textbackslash epsilon\^2\}\textbackslash right)\$, where \$d\$ is the dimension of the underlying space. For the second-order method (underdamped Langevin MCMC), the time complexity is \$\textbackslash tilde\{\textbackslash mathcal\{O\}\}\textbackslash left(e\^\{cLR\^2\}\textbackslash frac\{\textbackslash sqrt\{d\}\}\{\textbackslash epsilon\}\textbackslash right)\$ for some explicit positive constant \$c\$. Surprisingly, the convergence rate is only polynomial in the dimension \$d\$ and the target accuracy \$\textbackslash epsilon\$. It is however exponential in the problem parameter \$LR\^2\$, which is a measure of non-logconcavity of the target distribution.},
  archivePrefix = {arXiv},
  eprint = {1805.01648},
  eprinttype = {arxiv},
  file = {/Users/tmpethick/Zotero/storage/YI8AYLKF/Cheng et al. - 2019 - Sharp Convergence Rates for Langevin Dynamics in t.pdf},
  journal = {arXiv:1805.01648 [cs, math, stat]},
  keywords = {Computer Science - Machine Learning,Mathematics - Probability,Statistics - Computation,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, math, stat}
}

@article{chengUnderdampedLangevinMCMC2018,
  title = {Underdamped {{Langevin MCMC}}: {{A}} Non-Asymptotic Analysis},
  shorttitle = {Underdamped {{Langevin MCMC}}},
  author = {Cheng, Xiang and Chatterji, Niladri S. and Bartlett, Peter L. and Jordan, Michael I.},
  year = {2018},
  month = jan,
  abstract = {We study the underdamped Langevin diffusion when the log of the target distribution is smooth and strongly concave. We present a MCMC algorith{$\surd$}m based on its discretization and show that it achieves {$\epsilon$} error (in 2-Wasserstein distance) in O( d/{$\epsilon$}) steps. This is a significant improvement over the best known rate for overdamped Langevin MCMC, which is O(d/{$\epsilon$}2) steps under the same smoothness/concavity assumptions.},
  archivePrefix = {arXiv},
  eprint = {1707.03663},
  eprinttype = {arxiv},
  file = {/Users/tmpethick/Zotero/storage/6GWUUUJM/Cheng et al. - 2018 - Underdamped Langevin MCMC A non-asymptotic analys.pdf},
  journal = {arXiv:1707.03663 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{dalalyanSamplingLogconcaveDensity2018,
  title = {On Sampling from a Log-Concave Density Using Kinetic {{Langevin}} Diffusions},
  author = {Dalalyan, Arnak S. and {Riou-Durand}, Lionel},
  year = {2018},
  month = dec,
  abstract = {Langevin diffusion processes and their discretizations are often used for sampling from a target density. The most convenient framework for assessing the quality of such a sampling scheme corresponds to smooth and strongly log-concave densities defined on Rp. The present work focuses on this framework and studies the behavior of the Monte Carlo algorithm based on discretizations of the kinetic Langevin diffusion. We first prove the geometric mixing property of the kinetic Langevin diffusion with a mixing rate that is optimal in terms of its dependence on the condition number. We then use this result for obtaining improved guarantees of sampling using the kinetic Langevin Monte Carlo method, when the quality of sampling is measured by the Wasserstein distance. We also consider the situation where the Hessian of the log-density of the target distribution is Lipschitzcontinuous. In this case, we introduce a new discretization of the kinetic Langevin diffusion and prove that this leads to a substantial improvement of the upper bound on the sampling error measured in Wasserstein distance.},
  archivePrefix = {arXiv},
  eprint = {1807.09382},
  eprinttype = {arxiv},
  file = {/Users/tmpethick/Zotero/storage/6364IG9E/Dalalyan and Riou-Durand - 2018 - On sampling from a log-concave density using kinet.pdf},
  journal = {arXiv:1807.09382 [cs, math, stat]},
  keywords = {Computer Science - Machine Learning,Mathematics - Probability,Mathematics - Statistics Theory},
  language = {en},
  primaryClass = {cs, math, stat}
}


