
TODO (Konstantin):
- Regret bound for Bandit (make overview)
- Explain Hedge argument precisely (the lifting trick to probability dist)

# Introduction

- Today we will cover non-convex setting.
- The more complicated Bandit setting.
- First derive Hedge which will turn out to be directly applicable to the bandit setting.

- We will get results for infinite arm Bandits. Crucially we will introduce known adversarial actions but let us wait with that.
  $$f_i(a_i) = r(a_i, a^{-i})$$

# Expert problem (Hedge)

- Lagrangian
https://haipeng-luo.net/courses/CSCI699/lecture3.pdf
https://people.cs.umass.edu/~akshay/courses/cs690m/files/lec15.pdf

$$\mathcal{R}_{T}=\sum_{t=1}^{T}\left\langle p_{t}, \ell_{t}\right\rangle-\min _{p \in \Delta(N)} \sum_{t=1}^{T}\left\langle p, \ell_{t}\right\rangle=\sum_{t=1}^{T}\left\langle p_{t}, \ell_{t}\right\rangle-\sum_{t=1}^{T} \ell_{t}\left(i^{\star}\right)$$

with $i^{\star} \in \operatorname{argmin}_{i} \sum_{t=1}^{T} \ell_{t}(i)$.

https://haipeng-luo.net/courses/CSCI699/lecture2.pdf

<!-- Duality https://www.cs.huji.ac.il/~shais/papers/OLsurvey.pdf -->

# Multi-armed bandit (Exp3)

Hedge/Exp3 for multi-armed bandits.
based on notes: https://haipeng-luo.net/courses/CSCI699/lecture12.pdf
https://haipeng-luo.net/courses/CSCI699_2019/lecture8.pdf

def: Bandit setting

$$\mathbb{E}\left[\mathcal{R}_{T}\right]=\mathbb{E}\left[\sum_{t=1}^{T} \ell_{t}\left(a_{t}\right)\right]-\min _{a \in[K]} \sum_{t=1}^{T} \ell_{t}(a)$$

- Expectation -> relax to probability
- Becomes linear
* general trick

## Importance sampling approach

## Exp3 as modified Hedge

- write algorithm


# GP-MW

- Bound with UCB
- Split variance term
- We can bound this if we consider full information.
* general trick (also applied to RL https://arxiv.org/pdf/1911.05873.pdf)

- min{1, UCB}
- max -> min
- Discretization
- Introduce agents to bound information gain
