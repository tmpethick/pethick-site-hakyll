<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>Provably Beneficial Artificial Intelligence by Stuart Russell</title>

  <!-- CSS -->

  <link href="https://fonts.googleapis.com/css?family=Sorts+Mill+Goudy:400,400i&display=swap&subset=latin-ext" rel="stylesheet">
  <link rel="stylesheet" href="../../css/poole.css">
  <link rel="stylesheet" href="../../css/syntax.css">
  <link rel="stylesheet" href="../../css/hyde.css">
  <link rel="stylesheet" type="text/css" href="../../css/custom.css" />
  <!-- <link rel="stylesheet" type="text/css" href="/css/old.css" /> -->
  <!-- <link rel="stylesheet" type="text/css" href="/css/default.css" /> -->
  <!-- <link rel="stylesheet" type="text/css" href="/css/github.css" /> -->

  <!-- Icons -->
  <!-- <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="favicon.ico"> -->

  <!-- RSS -->
  <!-- <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml"> -->

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-120674790-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'UA-120674790-1');
  </script>
</head>


  <body>

    <div class="sidebar">
  <div class="container">
    <div class="sidebar-about">
      <a href="../../">
        Thomas
        Pethick
      </a>
      <!-- <p class="lead">Notes on some things.</p> -->
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item" href="../../about">About</a>
      <a class="sidebar-nav-item" href="../../archive">Archive</a>
    </nav>

    <!-- <p>Site generated by <a href="http://jaspervdj.be/hakyll">Hakyll</a>.</p> -->
  </div>
</div>


    <div class="content container">
      <div class="post hyphenate">
    <h1 class="post-title">Provably Beneficial Artificial Intelligence by Stuart Russell</h1>
    <span class="post-date">Posted on November  3, 2019</span>
    <p>Stuart Russell came to EPFL last Friday to talk about his new book on <a href="https://www.amazon.com/Human-Compatible-Artificial-Intelligence-Problem/dp/0525558616">Human Compatible: Artificial Intelligence and the Problem of Control</a>. Broadly speaking there were two main messages: first we should avoid specifying fixed objectives and instead learn them in a dynamic environment where the agent adhere to the human. His second main talking point was on the problem of how we aggregate utility — a similar problem to what the Effective Altruism movement face. I will expand on these in a bit but let’s first look at how he opened.</p>
<section id="sec:a-confused-kicker" class="level4">
<h5>A confused kicker</h5>
<p>He had a catchy opening illustrating the lack of robustness in modern RL methods. It showed two adversary player’s: one a goal keeper and the other the kicker. He showed that a seemingly useless (but hilarious) strategy of the goal keeper could completely confuse the kickers ability to score. What we end up looking at is a goal keeper who simply lies down and wiggle’s it’s leg. This is apparently sufficient to render the kicker utterly incapable — the agent will indecisively dance around the ball. With a well-behaved opponent the kicker motorics and strategy is very human, but this lack of robustness in an adversarial setting shows that something very different is going on under the hood. As Stuart Russell puts it: the two agents are intertwine in some intricate dance of tango. Now, of course in this zero-sum game dependency is inherent. The problem is that most of the training signal of the kicker seems to depend on the actions of the opponent instead of treating it as noise. What I could imagine is happening is the following: by actively not having the goal keeper search for an optimal strategy we make it harder to converged to a stable strategy.</p>
</section>
<section id="sec:sample-efficiency" class="level4">
<h5>Sample efficiency</h5>
<p>In effect it boils down to being much less sample efficient when the opponent act adversarially. That is, it requires many more samples from the environment before convergence to a stable solution. Here he drew a comparison with humans who are masters at few-shot learning<span><label for="sn-1" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-1" class="margin-toggle" /><span class="sidenote">He had a nice picture of this: children don’t scroll through a book with thousands of picture of giraffe’s to understand the concept of a giraffe.<br />
<br />
</span></span> — they learn classification tasks and master complicated decision processes with only a few training examples. He never connected the two but my interpretation is that robustness is essential to achieve this human level ability. This is simply because robustness will give you sample efficiency which corresponds to few-shot learning.</p>
</section>
<section id="sec:motivation" class="level4">
<h5>Motivation</h5>
<p>This picturesque introduction showed how brittle these systems are and how that can lead to unwanted behavior. He then switched gears to argue that a system, which on the contrary is <em>not</em> stupid<span><label for="sn-2" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-2" class="margin-toggle" /><span class="sidenote">He had one casual remark I found very provocative. When arguing for how great a personal assistant could get he said something along the lines of: “It will know what you want from listening in on your conversation and from reading your emails”. He said this seemingly seriously optimistic about what could turn into an uncontrolled surveillance system.<br />
<br />
</span></span>, could still exhibit undesired behavior. Assuming we could build capable systems, why would we have to control them?<span><label for="sn-3" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-3" class="margin-toggle" /><span class="sidenote">To me this seems almost unnecessary arguing for.<br />
<br />
</span></span> The strongest and most interesting<span><label for="sn-4" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-4" class="margin-toggle" /><span class="sidenote">This is interesting in its own right: it shows that AI systems and existing complex decision making processes could be treated similarly. We’ll touch on this further down the post.<br />
<br />
</span></span> argument for me was the parallel to nuclear power: ensuring that machines with a super human level of intelligence are safe is not so different from ensuring safety of existing powerful complex systems — in particular nuclear power plans. He continued by saying that the power of the system is not problematic in itself: it is only a problem if the objective of the system is misaligned with us.</p>
</section>
<section id="sec:misspecified-objective" class="level1">
<h2>2 Misspecified Objective</h2>
<p>This is often referred to as the <em>alignment problem</em> in AI. To clarify how difficult it is to define the right objective he took a particular view on what he referred to as <em>the social media catastrophe</em>. As I understand it, this is the problem of social media advocating extreme polarized opinions and misinformation. He explained this by first pointing out that the system benevolently were optimizing for good recommendation<span><label for="sn-5" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-5" class="margin-toggle" /><span class="sidenote">He later did point out that what they truly optimize for is profit. But this benevolent view makes it clearer that even with a well-intended objective it can go wrong.<br />
<br />
</span></span>. However, it turned out that it was an easier optimization problem to modify people’s behavior to be more predictable instead of learning their original objective. Thus polarizing peoples opinion gave a stable solution that could be gamed. Whether this is truly what happened is not entire clear, but it does show why optimizing in a dynamic environment is hard.</p>
<section id="sec:cooperation-as-complex-agents" class="level4">
<h5>Cooperation as complex agents</h5>
<p>At this point he took an interesting take on the corporations involved in this catastophe. He viewed the whole corporation as a big complex decision making algorithm — which crucially couldn’t be switched off. Through that, he motivated why any intelligent system should allow itself to be switched off. However, he only used the view to make this one argument, but failed to comment on the importance of the connection in itself. On a high level, AI and the complex decision making systems we already have can be treated the same and studied under the same umbrella. It suggest that studying safety in AI can help us regulate existing systems such as social economic structures. Conversely maybe we could use existing studies to guide safety research in AI.</p>
</section>
<section id="sec:solutions" class="level4">
<h5>Solutions</h5>
<p>Broadly speaking the social media example captures the two main problems: 1) we need to be able to switch the system off; and 2) we cannot assume that the objective can be specified completely and correctly a priori<span><label for="sn-6" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-6" class="margin-toggle" /><span class="sidenote">As an aside, he also mentioned how systems should be <em>minimally invasive</em>. That is, don’t for example destroy the town to bring coffee to the human. But this does not really provide many guarantees. In addition, this property is to a certain extend already taken care of in most algorithms since they usually have a level of greediness. The cheapest solution to compute will often also be the less involved.<br />
<br />
</span></span>. We will now touch on how he modelled this.</p>
</section>
<section id="sec:assistance-games" class="level4">
<h5>Assistance Games</h5>
<p>He modelled the problem as a two-player game with a human and a robot. The human has an objective <img width="10" alt="\mathbf \theta" height="12" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABUAAAAZCAQAAAB03K1mAAAA1ElEQVR4nI1TARHDIBB7C1iohVnAQi1goRawMAtYwAIWagELLNCV48c/Xbjr9SENIX2INBiyGH/AUySHZ8InS7w7JeB9Scxdy1OhTSMeWHS9sqxi2KB4DnWlem1zrrKjDrJmGXzeXqOmyU/sp5kGg+mCLUdE2atrVB55kRMIky/bqMLvzW1hHhNegkKUo3KCguK0xpLYzC4cs2HOdJ4ZfPEEs9ZV4Sd+p2leXsfzn1BVOtUxaq0OmXh11e3VQPHhooQvMWIsr59Brh5bJ63vOerdV/Q++0Bj8iTVxcQAAAAASUVORK5CYII=" class="inline-math" style="margin:0; vertical-align:-1px;" /> which can only be observed by the robot through play. The robot starts with some prior <img width="30" alt="p(\mathbf \theta)" height="17" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAD0AAAAiCAQAAABG6xksAAACUUlEQVR4nK1XCZWDMBCNhVioBSxgAQuxEAuxEAtYwEItYAEL7EyANnOFtN3f93hLjvlzM+vctxhc6jzp4aT/mkchXkxx3o3wqxHc87/IB7e6h7GXQKkAT0oW3fwfxB6Io7GXX5Qz/F1j6Q5QA2iXRby9bE1uJ555wPvwGzGKGNWdCDvh9TaSt0MxS+VOoEt1lTYIxBtITV08mkp3wcN1Pc6ZWTnBO0+t9ZdkiyyCFzAMG8lpjDV3MK59XWRP4tQ30Gaa0UmsHJ4ITgXW47Xly1siNnpF3LW+g+Aai4i1fR8OxlKz6KYRnlNRhmetVp2hUFNX7qqFq5ak/oxMKuLfumGtXpYnI0dnEdexUMuz6AuBeOrIxYQqpy3qrRDJn0TWGst8Omxlm2OlyqJm6KBYuCilZSp/xPAoEn64ppYIioV6pG2/nWKotvmWGsXR5JmUtLulzqJbPauC0KllTcuVDmoe6QdxnU4tK3gzel6DWkY6kAap5mepiYnd0W1uUMtIU4v0i3x1rToBh14j7rCqdh2WzVod1XtwINRBZAul3vSNlbkKD/LIy0Zar3q4YTkbIb9mLxEbOPxyVmbEh3La1XyGCZuxPalaqrsr0o8yUQZ4ZiViWc1xDyWYysjbHv2iNZ+9axrnaD1RJvFxvIB37saA2fjai5q2TrVi2YI5XMma1hG+HnIiG6GIyL6h7S6iOjD3jaLjNW1jbLQMG0kfolPpSPuZpz1iPh3mB0vdCFmLGYoq6PnLkRsdSwKL74fhX5L3+OfA8sHZLvT+8+Z1V/8Btfgf70G1BH4AAAAASUVORK5CYII=" class="inline-math" style="margin:0; vertical-align:-5px;" /> over this unknown objective and then tries to both learn and optimize the true objective<span><label for="sn-7" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-7" class="margin-toggle" /><span class="sidenote">To me this seems very closely related to <em>inverse reinforcement learning</em> in which the reward function is unobservable but trajectories from a <em>teacher</em> with an optimal policy are accessible.<br />
<br />
</span></span>.</p>
</section>
<section id="sec:image-classification" class="level4">
<h5>Image classification</h5>
<p>Even for static tasks he argued that a dynamically learned loss is needed. Current models assumes uniform loss even though this is clearly not the case. To make his point clear he picked the example of Google wrongly classifying a human as a gorilla. The cost of patching up their brand after this mistake was clearly out of proportions with any other misclassification they could have made. As a research direction he proposed finding the underlying structure in these losses (since they clearly exists) and doing this through active learning. In generally he suggested develop dynamic, or dynamically learned, losses in all current applications.</p>
</section>
<section id="sec:online-convex-optimization" class="level4">
<h5>Online Convex Optimization</h5>
<p>At the moment I’m instantly seeing everything as an Online Convex Optimization problem. In this setting we exactly avoid specifying a fixed objective — on the contrary the environment gets to choose the stream of losses adversarially. In the case of an unknown objective the loss is not really changing adversarially. Rather, it is our <em>estimate</em> of the loss which changes over time and the challenge seems to be that we have <em>limited access</em> to the loss, which is something considered in the Multi-armed Bandit setting. So it does not seem to be the usual online setting but some of the tools might still be useful in the analysis of Assistance Games — at least the notion of regularization and robustness.</p>
</section>
</section>
<section id="sec:aggregating-utility" class="level1">
<h2>3 Aggregating Utility</h2>
<p>The second main topic he touched on was the problem of aggregating utility. There were three rather disjoint comments on this.</p>
<section id="sec:pareto-paper" class="level4">
<h5>Pareto paper</h5>
<p>Imagine an agent optimizing for a group of principals who have different utility functions. Usually we are interested in a Pareto optimal policy. That is, a policy where no principal can improve without making sacrifices for one of the others. Harsanyi’s theorem shows that when the principals share a common prior of the environment dynamics then the Pareto optimal strategy for the agent is a fixed linear combination of all the principals utilities. What Russell and his colleges shows in <span class="citation" data-cites="desaiNegotiableReinforcementLearning2018">[1]</span> is that in the more general setting more weight should be given to the principle that turns out to be right. Exactly what to make of this theoretical result is a little unsure.</p>
</section>
<section id="sec:sadism" class="level4">
<h5>Sadism</h5>
<p>Assume that Alice’s utility can be described by a linear combination of her own satisfaction <img width="23" alt="w_A" height="10" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAC4AAAAUCAQAAABYUcd3AAABRklEQVR4nKVUWwHDIAyMBSxgoRawUAtYqAUszAIWsICFWcBCl2QtC2mh63Z88ApX7pICIGEhYHPQQs4XbD9hgoT0DlaYxeoD534bexyvGHcbBqkN9yuOPiiCbuKZVvYFlu1+c0M+4aw0UT+R560nGz6+ElkUUaTLdjliby/Uw/J4bD5FKNBDuFLl1U2LSqBpdiUsxw5rKYraODpOGekdj2xo6FO/TTF1rh0n6eeuzlwQOlqFrDWxhMdBaIIzGF53qogVghKWVIqmjuzAeTEHE4fkpTGJlJyZ4uoZMrUL35Bb9bP37h3rBfLocTDwxLaHJk5oqHuxUbFDPnTaRgV6vDIq8NjTHRa0JrDwc2rLSdzbZaUDv4yuUhme9cS2n9Q5+wte3XMeV/odmENVX1T6HeosHoo37LjSv0XiMi2CnnQ8eTXvvr8AmeCIZ9HXOxEAAAAASUVORK5CYII=" class="inline-math" style="margin:0; vertical-align:-3px;" /> and her friends, Bob’s, satisfaction <img width="23" alt="w_B" height="10" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAC8AAAAUCAQAAAC3k6xJAAABVElEQVR4nK1UUQGEMAilwiqswipY4SpYwQpUsIIVVsEKVlgFD6Y3GUzvzrvHh6LbA94YABIekKyDGtIfyG4iQKQAHazwEF9H8vv9vaf3ldbdgCNyl58rvR1IgjBkT1f3EYY9x0dFH8hL1aqb9PP+ZDEOfZluEqu4Nn+HHst2STBVwRiyFk91zrm+WGyhBE8T6FW2SR2lq/4yWEysQiYy16afRJ9Y5ZlMNyaa00BTccmNpTkia+V5qy6cxbEBERrgQmfhjyaPWG8wbcxfWJym+jpuVIUHk5VW3lF66eziafpUSQWNnth07nYbiPxN3xz0Xo0Am/umPAqLojEMHHXtUvKN+Wix/JtMu1nleWKN5wFCvig9Wcx5DyQQz1BskFvlX/Vcjg2fdXyRuey1D8v2/NYOFwJ9A9vzeqj8AKv8duubV+p72GkT/0XOLci3YhGTcrEj4wmGMJwxjEWUigAAAABJRU5ErkJggg==" class="inline-math" style="margin:0; vertical-align:-3px;" />,</p>
<p><img width="140" alt="u_A = w_A + C_{AB} w_B." height="14" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAARkAAAAdCAQAAAA0/9A+AAAEMklEQVR4nO1bbYGkMAythVrAAhawgAUsYAELYwELWMACFrDAtYGbadOvBEp3b47XH3uzMOkjpMlreivEg1KoRaNGjX734IEHrRjFKhYxwVjEcPy+E68f5fXgV6JRIbKp0PjkE6lC5gVXNhU0Dx4Y0MGxqbxSOVd00KzqmnvlwX8MKWbIL37o3LMU5fOgINYTMjUeMALyz6Nkvhab0h1czFCSwmgfJfPN4IeMziFrVKk0j5L5ZnBDRofD9t5Kh+75CiVTqcccHPfUv2o1+Dman3s18oIbMhPkGBm9p7lFyRT2Tg3bQb1CWotEvCaXhZ/jy9AFHazwvF1VXsjsOeYnpG1h70g1nYSfdoj0v0jZhziuhhtq+MSXqzHw7L3gtbTpGzOjuHf6IxJbNOGYRdnvVGkj7Gw/xxrKgHnXz4bM/qTlUdw78/FTrxGz2lF6lCNB7TTkEdYAfo7aCaNxl4wypnDF4Di5goC5VsrPcMzjHRaGt0HTZE3oUQ7ZV3V4Jg3McURBLqxVhS2c4cr5DmW3FIefY6XCcIaMMb3HosKjMr6nwfNO2moSHYrJtJKpIA/l3qPEgDmatVpDWldNpLlW4EDKWAIFlBYyYQEa49giy/pevDPje4diNQKsXNJKZoREeGVVcWFzwrVauyAUFBSuvlKpXyG1gNJCJly4Yhzd/DM44cX3DsVqEHtSM12RUjItiK4tuK7zA3PEtVo/sJ/xea65C1MbvB7nODuyekBznfFO2moEOkXNxmc3Rm1IWC0NQe5VRq1Mjfj2FHPEcj20gulcXfB3THGHj4EMFeeIN8/6Nys6luB7h2I1AhxdHyXjT/YDVEmZCKydRg99ScqIV1HMcUIvtA4mdSpXF/y+zBy5Hu6/xjlizaHPyvEZO987FKsR4Ak/anvwGGne95bsQ2COKyqlfrV/jSsvZPZtdugb4YOCFMfB0lS9erXus/K9Q7G6s/OityasjUf3JcpPep1F7hZ9GJ3FcX89n7lDOeYaV+7GfIA848uWXbAopTnO8OSfMXk2JnzvUKzud3kzp3meJFW0vQ5nNZ6pzMMvnP7uhFTb2+Xt3MkKcxl4IVe58r+xFyd7tVaKXVjjpDi6mqMRbgOE6x2q1S2cnTuVyl4QItq8hDTVHWcXJvbg+jvKdmZqaD91wEuvoF7NP7w5u7jO9cyCaOG/ao5H0u/Vv6aIlTRHrDk0Zg8znneoVhPNCfuvbRrvg9rTMzZlmVBZnRGJONu4zvVsDt1f1KQc3id2IGmOvp7wFOia0b3DsXoJHVoDbdHODA85uN5ddikc3e4JPho4g3usOpCOGD7X7SiBPFx7evv8BCgcXc2x72ev5fZ7rHqmmZ20VZ3qdtyPf4ErjaN7DjRleLX3WEWYIG2txkNKOPvcYIdQVs+k8C9wpXAcDkG8vOXxAn+ke614nLb6Bzao7oN90cI0AAAAAElFTkSuQmCC" class="display-math" style="margin:0; vertical-align:-3px;" /></p>
<p>If <img width="32" alt="C_{AB}" height="14" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAEEAAAAdCAQAAABfsslCAAABxElEQVR4nMVXURGDMAythVrAAhZqoRawgAUsYAELtVALWKgFlqQcMJIU7kZZ+rNroXnJe0mYMdfWGgerPe29ZN5MJpnZBFqzGdb9zoxvuHfgcgFXe7wWIIx0sgCIyobOFoi7YScIIsEZP3kYQKT4ZcPczP8EYCg/lZUQiQLdfG0lYIypyLSrqwS8ftlKT3umqhIC5cBeQKiohJyDV5qOZiNB8P+EkAjCH60hAKVyvLbpt1q5Uw1lw4J2bLeBsCLJPGxrBtIFsPcg6EO6ISJ78cyfbsZnhcq7B0EnaiI5y+/z/AwS3DsQvHru4cIO3p/E08hkPsi+0iWESWlblrLjFDlbtm+JCEENmMhYANArTGNM7eoqCadnJeAsTrKqcllyTWfTG7PbHMh9JfPu1tUDALEe9oejmOxOJeFID3LOo4uUhX2F8rDPZHxjbMCJrpHhkLcgZJErwV1NIk+fZtOauh5+BZWcTN7ecqTOcFYCWiwQvuEcyPUIF5Zb7jc9UqlJPTM89+XVnWL2QmfgPcGSbB/58rKsC/DOwJWAmfttFh0ujyyZDesMfDqEpwAESmY6gLA0AReqqOxiWAU6HyYk/ik8UfABnMDdRQ1B2XEAAAAASUVORK5CYII=" class="inline-math" style="margin:0; vertical-align:-3px;" /> is negative Ali is sadistic since she obtain pleasure from the dissatisfaction of Bob. This raises the question of what we should optimize for. A simple solution would be to completely disregard the preference of sadistic people. However, sadistic people are not so uncommon when we realise that Pride and Rivalry would have the exact same mathematical description<span><label for="sn-8" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-8" class="margin-toggle" /><span class="sidenote">I am not too sure I see how this complicates matters since we could simply ignore any negative coefficient <img width="32" alt="C_{AB}" height="14" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAEEAAAAdCAQAAABfsslCAAABxElEQVR4nMVXURGDMAythVrAAhZqoRawgAUsYAELtVALWKgFlqQcMJIU7kZZ+rNroXnJe0mYMdfWGgerPe29ZN5MJpnZBFqzGdb9zoxvuHfgcgFXe7wWIIx0sgCIyobOFoi7YScIIsEZP3kYQKT4ZcPczP8EYCg/lZUQiQLdfG0lYIypyLSrqwS8ftlKT3umqhIC5cBeQKiohJyDV5qOZiNB8P+EkAjCH60hAKVyvLbpt1q5Uw1lw4J2bLeBsCLJPGxrBtIFsPcg6EO6ISJ78cyfbsZnhcq7B0EnaiI5y+/z/AwS3DsQvHru4cIO3p/E08hkPsi+0iWESWlblrLjFDlbtm+JCEENmMhYANArTGNM7eoqCadnJeAsTrKqcllyTWfTG7PbHMh9JfPu1tUDALEe9oejmOxOJeFID3LOo4uUhX2F8rDPZHxjbMCJrpHhkLcgZJErwV1NIk+fZtOauh5+BZWcTN7ecqTOcFYCWiwQvuEcyPUIF5Zb7jc9UqlJPTM89+XVnWL2QmfgPcGSbB/58rKsC/DOwJWAmfttFh0ujyyZDesMfDqEpwAESmY6gLA0AReqqOxiWAU6HyYk/ik8UfABnMDdRQ1B2XEAAAAASUVORK5CYII=" class="inline-math" style="margin:0; vertical-align:-3px;" />. I realise that in a simplified static world this is suboptimal. But we have to acknowledge that peoples preferences will update and ethically I think we can agree that we want to stir away from pride and rivalry. If we do not support this kind of utility in our reward system there a higher chance it will diminish.<br />
<br />
</span></span>.</p>
</section>
<section id="sec:optimizing-jointly" class="level4">
<h5>Optimizing jointly</h5>
<p>As a final example on problems with optimizing a global utility he gave a comical image: picture a kitchen robot denying its owner food because its decided that there are more urgent matters in some third-world country. This capture more of an engineering challenge and to me seem very flawed. It is misleading since the optimization process would be jointly but the <em>action space</em> considered would be for each robot individually. So it does not seem like a real issues: neither theoretical nor practically.</p>
</section>
</section>
<section id="sec:final-remarks" class="level1">
<h2>4 Final remarks</h2>
<p>Being careful with what we are optimizing for definitely seems crucial. This is both in terms of learning or adjusting to a changing objective and understanding what we implicitly assume when aggregating the utility. Further, I don’t see why these questions are only relevant within AI safety. Effective Altruism is about optimizing the optimization process and here a proper definition of utility is even more important. Even more broadly these concerns apply just as much to any far-reaching decision making body — be it a state, a multinational company or the scientific enterprise.</p>
<div id="refs" class="references">
<div id="ref-desaiNegotiableReinforcementLearning2018">
<p>[1] N. Desai, A. Critch, and S. J. Russell, “Negotiable Reinforcement Learning for Pareto Optimal Sequential Decision-Making,” in <em>Advances in Neural Information Processing Systems 31</em>, S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, Eds. Curran Associates, Inc., 2018, pp. 4712–4720 [Online]. Available: <a href="http://papers.nips.cc/paper/7721-negotiable-reinforcement-learning-for-pareto-optimal-sequential-decision-making.pdf" class="uri">http://papers.nips.cc/paper/7721-negotiable-reinforcement-learning-for-pareto-optimal-sequential-decision-making.pdf</a>. [Accessed: 03-Nov-2019]</p>
</div>
</div>
</section>
</div>

    </div>

  </body>
</html>
