---------
title: Gradient play in two-player games
---------


<figure class="figure-fullwidth">
<div id="contour-top"></div>
<figcaption></figcaption>
</figure>

This is an attempt at illustrating the dynamics of different scheme for finding solutions to zero-sum two-player games.
It relies on the fact that many of the issues in these games can be captured in very simple 2-dimensional cases.

# Solution concept

Let say we are interested in an adversarial game between two players where one aims at minimizing a function while the other attempts at maximize that same function

$$\min_x \max_y f(x,y).$$

Interest in this problem is primarily driven in the machine learning community by reinforcement learning and training of Generative Adversarial Networks (GANs).

To approach the problem we need some requirement of a solution. 
A natural solution concept is to consider a notion a stability by requiring that no agent would change its strategy unilaterally, i.e. without the other agent changing strategy.
We say that the tuple of strategies $(x^*,y^*)$ is a _Nash equilibrium_.
This is a rather strong requirement if we allow $f$ to be non-convex, since the problem would be NP-hard.
So, like in non-convex minimization, it is common to instead seek a local variant as made precise below.

<div class="block--def">
*(Local nash equilibrium)* A solution tuple $\left(x^{*}, y^{*}\right)$ is a local nash equilibrium (LNE) if 

$$f\left(x^{*}, y\right) \leq f\left(x^{*}, y^{*}\right) \leq f\left(x, y^{*}\right)$$

for $x$ and $y$ within some neighborhood of $x^*$ and $y^*$ respectively, i.e. $\left\|{x}-{x}^{\star}\right\| \leq \delta$ and $\left\|{y}-{y}^{\star}\right\|$ for some $\delta>0$.
</div>
In words, this states that neither players have incentive to deviate unilaterally from their current strategy when restricted to making small changes.

Now, how do we find such a solution?

# Bilinear games

A running example seems appropriate.
Even before considering the general case of non-convex $f$ it will be illuminating to study the deceivingly simple example of a strictly competitive bilinear game

$$f(x,y)=x^\top Ay.$$

The first thing to verify is that is has a local nash equilibrium.
It it easy to see that at $(0,0)$ there is no interest in deviating for either players.
So the origin is a LNE (and in fact also a global NE)[^1].

[^1]: <span id="contour-nash-eq"></span>
    A bilinear game has a nash equilibrium.

# PPM

We can now consider what iterative scheme to run to find the LNE.
Since the gradient is our only access to $f$, the gradient of both players will be important, so it is useful to consider the stacked vector

$$\omega(z)=(\nabla_x f(x,y), -\nabla_y f(x,y))^\top \quad \text{with} \quad z=(x,y)^\top.$$

Given this we can write the (probably most obvious) suggestion by recalling the workhorse of minimization: gradient descent (GD)

$$z_{k+1} = z_k - \eta \omega(z_k).$$

Unfortunately this falls prey...

- Acceleration even worse
- PPM: Acceleration vs OGDA (inwards and outwards spiral)


<figure class="figure-fullwidth">
<div id="contour-PPM"></div>
<figcaption></figcaption>
</figure>

# Hamiltonian
- Hamiltonian: SGA/CO. We can converge fast. (introduce a game which is not hamiltonian to show convergence to non-LNE)

# LLS
- LSS avoids this

# Limit cycle
- Limit cycles: Still prone to limit cycles. A large class will converge to this.



It is well known that simGD fails in even bilinear games.

<figure class="figure-fullwidth">
<div id="contour-bilinear"></div>
<figcaption></figcaption>
</figure>

<!-- To circument this various methods have been proposed.
Most such methods fall under considering the stacked vector and using the hessian.

In non-convex setting this paper shows failure cases. -->

# For non-convex games

<!-- - How do we characterize the DNE? (in constrast with the other attractors)
- Why is this surprising given it is non-convex (for which GD would not work for optimization either)
- How do we construct unstable games (almost bilinear)
- How do we construct stable games but with shielding (forsaken) -->

<figure class="figure-fullwidth">
<div id="contour-stable"></div>
<figcaption></figcaption>
</figure>

<figure class="figure-fullwidth">
<div id="contour-unstable"></div>
<figcaption></figcaption>
</figure>

<!-- # Final remark

I will leave you with this curious piece drawn by simGD when the step size is pushed to the extreme. -->

<figure class="figure-fullwidth">
<div id="contour-wild"></div>
<figcaption></figcaption>
</figure>

<!-- -----

# Lookahead

Simulatous with this work: lookahead which seems to avoid the problems.

# Second order

HD:
https://arxiv.org/pdf/1906.02027.pdf
SGA:
http://proceedings.mlr.press/v80/balduzzi18a/balduzzi18a.pdf
Consensus optimization:
https://arxiv.org/pdf/1705.10461.pdf

Competitive GD (we need higher order information to not get trapped! The off-diagonal terms in the jacobian!):
https://arxiv.org/pdf/1905.12103.pdf
  
- In minimization we approximate with linearization regularized by a quadratic penalty that captures how confident we are in this approximations.
    
    $$x_{k+1}=\operatorname{argmin}_{x \in \mathbb{R}^{m}}\left(x^{\top}-x_{k}^{\top}\right) \nabla_{x} f\left(x_{k}\right)+\frac{1}{2 \eta}\left\|x-x_{k}\right\|^{2}$$

- _How to linearize a game?_
- Linear approximation of $f, g: \mathbb{R}^{m} \times \mathbb{R}^{n} \longrightarrow \mathbb{R}$ cannot capture this (cannot express interaction between players since linearity assumes independence).
- Bilinear approximation instead! [^1] -->


<script src="/projects/games/dist/js/app.bundle.js"></script>
